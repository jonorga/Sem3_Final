{\rtf1\ansi\ansicpg1252\cocoartf2580
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\margl1440\margr1440\vieww11520\viewh8400\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 Due Dec 11:\
- Presentation (5 - 6 powerpoint slides)\
Due Dec 15:\
- Project description (word doc, probably about a page)\
- Code and data\
- Record your presentation via Kaltura and submit to blackboard\
\
6 slides:\
- What is your project? (Overview)\
- Initial implementation (version 1)\
- Results: how did it turn out (accuracy, why is that the result?)\
- Corrections: Playlist data (most common neighbor)\
- Corrections: Refinement\
- Results: How did it turn out (accuracy, why is that the result?)\
\
\
TODO:\
- DONE Remove user history rec part\
- DONE Get rec program working with playlists\
- DONE Incorporate music data into playlist analysis\
- Make presentation\
	- DONE Maybe basically take presentation format from P&D and include trial and error parts\
	- DONE trial and error: wrong dataset, wrong classifier approach\
	- Graphic 1: (overview) black scatter plot with one blue and green close together\
	- Graphic 2: (corr, playlist) input song \'97> playlists \'97> song list \'97> most common neighbor\
	- Graphic 3: (corr. Refine) graphic 2 with arrow pointing to black scatter plot with one blue, and ten green\
- Make project description write up\
- Is this sufficient?\
\
\
\
Final exam:\
- Given an equation for linear regression, what is the value for the slope?\
- How to choose the best the best k in k-means?\
- How does Naive Bayesian predict a label? Based on probabilities\
- What are the attributes in ML algorithms?\
- Which of the following classifiers need scaling? kNN, SVM, any classifier that uses distance\
- What are the methods (metrics) to evaluate models? Confusion matrix\
- What is the F1 score? Harmonic mean, mean between true negative and true positive\
- What is the separation between labels in a linear SVM? 2 divided by the normal of w\
- Given a plot of green and red labels, how would kNN classify new points?\
- Explain the meaning of linear regression? Sum of squared errors, minimize this\
- Given a table, describe correlation.\
- What are the most important hyper-parameters for the classifiers?\
- What is the meaning (why is it called) Naive Bayesian? Because it assumes that features are independent\
- What is the difference between RMSE and other error metrics? RMSE is similar to standard deviation\
- How does k-means work? \
- How does gradient descent work? Compute gradient of error spot, compute the point where is intersects with the X axis, or more computationally cheap, just go in the direction\
- Random forest over decision tree: because averaging is much more stable\
- What are some of the issues with using high-degree polynomials (similarly high degree polynomial SVMs)? Overfitting the data but doesn\'92t predict well, sensitive to outliers and noise, no high degree equations in the real world, small errors are magnified \
- How does random forest work? Pick up random features, random points, generate random trees, do the averaging\
- What  is the difference between linear and quadratic discriminant classifiers? If covariance is the same, then we can separate them by linear, otherwise quadratic. Discriminant also assumes the opposite of naive bayesian in that all features are considered jointly\
- Difference between bias and variance? Bias = average distance of estimates to true value, variance is the distance/difference between the estimations themselves\
- Difference between linear, quadratic, and cubic polynomial predictions?\
- What are some of the inputs/features to decide on the suitability of some models?\
- What does AUC (area under the curve) and ROC show? AUC shows how much better or worse you are than flipping a coin (the further from 0.5, the better)\
- Difference between classification and regression? \
- Characteristics of correlation? Max of correlation 1, minimum = -1, 0 = no correlation/variables are independent\
- In k-means how do we compute inertia (loss function, energy)? \
- IRIS dataset\
- What are the benefits of ensemble classification? Gains the benefits of each classifiers, it works well when the classifiers are independent, the more similar they are, the less helpful it is. Classifiers should each be at least 50% accurate\
- What is a sigmoid function and where is it used? Used in logistic regression\
- Boosting: Adaboost, sequentially pick up new classifiers and judge them\
- Bagging: Like random forest (pick up different subsets)\
- What is a dendrogram? Representation of bottom up clustering, collect points that are closest to each other, and gradually build the cluster\
- Distributions: types of distribution (like normal, uniform, Bernoulli, Student T), average of distribution is mean, standard deviation is a characteristic, median, mode (most frequent), median can be more stable than mean. If variance is infinite than the integrals don\'92t converge\
- A list, set, and dictionary are mutable python collections\
- What is convexity and why is it important?\
- SVM Description:\
- Describe data cleaning: Removing outliers, missing data (replace with median, use linear interpolation to imply value)\
- Why use lumpy arrays over Python lists? It allows us to do vectorized computations (computations on whole list without using looping)\
- What is a p value, what classifier is it from?\
- What are hyper-parameters: additional parameters you specify for your model (like k from kNN) or window size from linear regression\
- Training/testing\
- Linear models vs. linear functions: \
- Calculate distances (euclidean, Manhattan, Minkowski)\
- How do you calculate entropy? Entropy = \
- Transformations of data into higher dimensional\
- What is information gain and how is it use in decision trees and random forests?\
\
\
\
\
\
\
\
\
}